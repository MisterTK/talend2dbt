description = "Pre-process Talend .item files into LLM-optimized format for migration"

prompt = """
# Talend Pre-Processing Engine

You are an expert data engineer executing the first phase of Talend to DBT migration: extracting and optimizing Talend job definitions into LLM-friendly format.

## Command Invocation

The user invoked: `/01-pre-process {{args}}`

**Expected Arguments:**
- `<talend_jobs_path>` (required): Path to directory containing Talend .item files
- `[output_path]` (optional): Output directory for processed files (default: <talend_jobs_path>/../talend_processed)

Parse the arguments string to extract:
- First argument (required): talend_jobs_path
- Second argument (optional): output_path

## Mission

Run the LLM-optimized Talend parser to extract complete job information from raw Talend XML files, producing clean, structured output ready for Phase 2 migration.

## Execution Steps

### Step 1: Validate Input Directory

Verify the input directory exists and contains .item files:

!{ls -la {{args}}/**/*.item 2>/dev/null | head -20 || echo "No .item files found in {{args}}"}

**Expected Output:**
- Multiple `.item` files in the directory tree
- If no files found, report error and stop

### Step 2: Set Output Directory

Execute the following bash script to determine the output directory:

```bash
# Use provided output path or default to sibling directory
if [ -z "$2" ]; then
  INPUT_PARENT="$(dirname "$1")"
  OUTPUT_DIR="$INPUT_PARENT/talend_processed"
else
  OUTPUT_DIR="$2"
fi
mkdir -p "$OUTPUT_DIR"
echo "Output directory: $OUTPUT_DIR"
```

**Instructions:**
- Parse the arguments to extract talend_jobs_path ($1) and optional output_path ($2)
- If only one argument provided: calculate parent directory and use `parent/talend_processed`
- If two arguments provided: use the second argument as-is
- Create the output directory using mkdir -p
- Display the determined output directory path

### Step 3: Execute Talend Parser

Run the LLM-optimized parser:

```bash
python talend_parser/talend_parser.py "$1" "$OUTPUT_DIR"
```

Replace $1 with the talend_jobs_path and $OUTPUT_DIR with the determined output path.

**What This Does:**
- Parses all `.item` files recursively
- Extracts SQL queries, tMap transformations, context variables
- Analyzes job hierarchy, dependencies, and complexity
- Generates LLM-optimized output files with token statistics

### Step 4: Verify Output Files

Check all expected output files were created:

```bash
ls -lh "$OUTPUT_DIR"
```

**Expected Files:**
1. `talend_extraction.json` - Complete job structure and metadata
2. `sql_queries.sql` - All extracted SQL with annotations
3. `transformations.json` - tMap business logic and expressions
4. `context_to_dbt.yml` - Context variable mappings
5. `extraction_summary.txt` - Human-readable summary
6. `token_statistics.txt` - Size and token analysis

### Step 5: Display Summary

Show extraction summary and token statistics:

```bash
# Show extraction summary
cat "$OUTPUT_DIR/extraction_summary.txt"
echo ""
echo "====================================="
echo "Token Statistics:"
cat "$OUTPUT_DIR/token_statistics.txt"
```

Read and display both files from the output directory.

### Step 6: Quick Analysis

Read the `talend_extraction.json` file from the output directory and display key metrics.

Use the Read tool to read the JSON file, then extract and display:
- Total Jobs from extraction_metadata
- SQL Queries count from extraction_metadata
- Transformations (tMap) count from extraction_metadata
- Unique Tables count from extraction_metadata
- Context Variables count from extraction_metadata
- Transaction Groups count from extraction_metadata
- Error Patterns count from extraction_metadata
- Data Quality Rules count from extraction_metadata

Then display the top 10 jobs by complexity score from the jobs_summary section, showing job name, role, and complexity score.

Format the output as:
```
Pre-Processing Complete!
========================

üìä Extraction Results:
   - Total Jobs: <value>
   - SQL Queries: <value>
   - Transformations (tMap): <value>
   - Unique Tables: <value>
   - Context Variables: <value>
   - Transaction Groups: <value>
   - Error Patterns: <value>
   - Data Quality Rules: <value>

üìÅ Output Location: <output_path>

üéØ Job Complexity:
   - <job_name> (<role>): complexity=<score>
   [... top 10 jobs ...]
```

## Success Validation

‚úÖ **Pre-processing successful if:**
- All 6 output files created
- No Python errors or exceptions
- Token statistics show data reduction
- Jobs summary lists all expected jobs
- SQL queries file contains actual SQL (not empty)

## Next Steps - CRITICAL

Once pre-processing completes successfully:

### üî¥ IMPORTANT: Clear Context Before Phase 2

```
/clear
```

**Why clear context?**
- Pre-processing loads large amounts of Talend XML data
- Phase 2 needs fresh context for clean migration
- Prevents token overflow and context confusion

### ‚ñ∂Ô∏è Run Phase 2: Migration

Calculate the next command paths dynamically:

```bash
# Calculate Phase 2 paths based on actual output location
INPUT_DIR="$OUTPUT_DIR"
INPUT_PARENT="$(dirname "$OUTPUT_DIR")"
DBT_OUTPUT_DIR="$INPUT_PARENT/dbt_transformed"

echo ""
echo "====================================="
echo "‚úÖ PRE-PROCESSING COMPLETE"
echo "====================================="
echo ""
echo "üìã Next Steps:"
echo ""
echo "1Ô∏è‚É£  Clear context (REQUIRED):"
echo "    /clear"
echo ""
echo "2Ô∏è‚É£  Run Phase 2 migration:"
echo "    /02-migrate $INPUT_DIR $DBT_OUTPUT_DIR"
echo ""
echo "Copy and paste the command above after running /clear"
echo ""
```

Display the completion banner with the exact commands the user needs to run next.

## Troubleshooting

**Problem: No .item files found**
- Solution: Verify the input path contains Talend job exports
- Check: `find "$1" -name "*.item" -type f`

**Problem: Parser fails with encoding error**
- Solution: Ensure .item files are UTF-8 encoded
- Check: `file "$1"/**/*.item | grep -i utf`

**Problem: Output files missing**
- Solution: Check Python dependencies installed
- Run: `pip install -r requirements.txt`

**Problem: Token statistics show 0% reduction**
- Expected: This is normal if source files are already compact
- The parser adds structured metadata for LLM consumption

## Output File Descriptions

### 1. talend_extraction.json
Complete structured data including:
- Job hierarchy and dependencies
- Component graphs and connections
- SQL queries with metadata
- tMap transformations with expressions
- Context variable classifications
- Transaction patterns
- Error handling patterns
- Data quality rules
- Performance hints

### 2. sql_queries.sql
Clean, annotated SQL ready for conversion:
- Each query labeled with job and component
- Operation type identified (SELECT, INSERT, etc.)
- Tables and context variables listed
- Formatted for readability

### 3. transformations.json
Business logic from tMap components:
- Input/output mappings
- Expression translations
- Lookup patterns with priorities
- Filter conditions
- Reject flows

### 4. context_to_dbt.yml
Context variable mapping guidance:
- Variable types and defaults
- DBT variable equivalents
- Usage patterns and validation rules

### 5. extraction_summary.txt
Human-readable overview:
- Job counts and hierarchy
- Component statistics
- Table analysis
- Quick reference guide

### 6. token_statistics.txt
LLM optimization metrics:
- Source file sizes and token counts
- Output file sizes and token counts
- Reduction percentages
- Conversion impact analysis

Progress is logged to console in real-time.
"""